 V2X-Reg++: A Real-time Global Registration Method for Multi-end
             Sensing System in Urban Intersections
  Xinyu Zhang1,2,3,4,∗ , Qianxin Qu1,∗ , Yijin Xiong1,∗,† , Chen Xia1 , Ziqiang Song1 , Qian Peng1 , Kang Liu1 , Jun
                                                  Li1 , Keqiang Li1




   Abstract—Urban intersections, dense with pedestrian and ve-
hicular traffic and compounded by positioning signal obstruc-
tions, are among the most challenging areas in urban traf-
fic systems. Traditional single-vehicle intelligence systems often
perform poorly in such environments due to a lack of global
scene observations and the inherent uncertainty in predicting
other agents’ intentions. Vehicle-to-Everything (V2X) technology,
through real-time communication between vehicles (V2V) and
vehicles to infrastructure (V2I), offers a robust solution. However,
practical applications still face numerous challenges. Spatial
registration among vehicle and infrastructure endpoints with
different configurations in multi-end sensing systems is crucial for
ensuring the accuracy of perception system data. Most existing
multi-end spatial registration methods rely on initial extrinsic
values provided by positioning systems, but the instability of
GNSS signals due to high buildings in urban canyons poses
severe challenges to these methods. To address this issue, this                Fig. 1: Schematic of extrinsic parameters challenges at urban
paper proposes a novel multi-end spatial registration method that              intersections.
does not require positioning priors to determine initial external
parameters and meets real-time requirements. Our method
introduces an innovative multi-end perception object association                  In complex environments, conventional intelligent trans-
technique that leverages a new Overall Distance (oDist) metric
to measure the spatial association between perception objects,                 portation systems often struggle to adapt due to a lack of a
subsequently using this metric as the foundation for an optimal                holistic view of traffic conditions and an inability to proac-
transport formulation. By this means, we can extract co-observed               tively model predict traffic flow evolution [2]. To address
targets from object association results for further external param-            these challenges, Vehicle-to-Everything (V2X) technology [3]
eter computation and optimization. Extensive comparative and                   integrates real-time data exchange between vehicles (V2V) [4]
ablation experiments conducted on the simulated dataset V2X-
Sim and the real dataset DAIR-V2X confirm the effectiveness                    and infrastructure (V2I) [5], enabling efficient management of
and efficiency of our method. The code for this method can be                  traffic scenarios, such as congestion and accidents [1], and
accessed at: https://github.com/MassimoQu/v2i-calib.                           significantly improving urban traffic safety and efficiency.
                                                                                  However, this technology also introduces new challenges,
                                                                               particularly in the areas of multi-sensor data fusion and reg-
                        I. INTRODUCTION                                        istration. In V2X systems, the multi-end sensing system, typ-
   Urban intersections, as key nodes of urban transportation                   ically consisting of multiple LiDAR devices [6]–[9], requires
networks, handle significant volumes of pedestrian and vehicle                 precise spatial registration to ensure data consistency and
traffic daily. Particularly in city centers and commercial dis-                accuracy. Due to the high dynamism of urban intersection sce-
tricts, the road junctions at these sites exhibit highly dynamic               narios, traditional static single-time spatial calibration methods
traffic flows, not only increasing the risk of traffic accidents               [10] no longer meet practical needs. Moreover, existing multi-
[1] but also posing substantial challenges to traditional traffic              end spatial registration techniques [8], [9], [11]–[15] generally
management systems.                                                            rely on positioning systems (e.g., GNSS) to provide high-
                                                                               precision initial extrinsic values. However, in practice, it is
   This research has been supported by the National Natural Science Founda-    challenging for positioning systems to consistently meet the
tion of China (Project No. 52221005; 62273198), and the Beijing Natural        requirement, limiting the applicability of such methods in real-
Science Foundation Program under Grant No.L241017, and the National
Research Foundation Singapore under its AI Singapore Programme (Award          world scenarios.
Number: AISG2-GC-2023-007).                                                       In urban environments, the urban canyon effect often causes
   ∗ These authors contributed equally to this work as co-first authors.
   † Corresponding author: Yijin Xiong(yj-xiong@mail.tsinghua.edu.cn).
                                                                               fluctuations in GNSS signals due to building obstructions and
   1 The State Key Laboratory of Automotive Safety and Energy, and the         signal reflections. This issue is particularly pronounced at
School of Vehicle and Mobility, Tsinghua University, Beijing, 100084, China.   urban intersections, where low vehicle speeds make it difficult
   2 Tsinghua Automotive Research Institute (Suzhou), Tsinghua University,
                                                                               to distinguish effective signals from interference [16], fur-
Suzhou, China.                                                                 ther exacerbating positioning instability. The US Department
   3 Electrical and Computer Engineering, National University of Singapore,
Singapore, 117583.                                                             of Transportation’s V2X project [17] identified insufficient
   4 Anhui Mengshi Aviation Technology Co., Ltd., Hefei, China.                positioning accuracy at intersections as a major challenge.
Fig. 2: The proposed method first generates 3D detection boxes on each endpoint, then identifies the common objects across
endpoints. This multi-end object association is achieved through two steps: spatial similarity evaluation and object matching.
The core process involves filtering the association of multi-end 3D detection boxes based on the affinity matrix (evaluated
through oDist Calculation, with visualizations of both the matrix and the association degree). After identifying the common
objects, feature point clouds are extracted from the detection boxes, and the extrinsic parameters are further computed using
weighted point cloud registration.


Related experiments [18] also revealed that “missing relative      Optimal Transport (OT) formulation to achieve robust object
positioning data” is notably more frequent in intersection         matching. Subsequently, the feature point clouds derived from
scenarios, highlighting the difficulty of ensuring reliable po-    these confidently matched objects are used in a weighted
sitioning accuracy even with advanced systems. In addition,        SVD algorithm, where weights reflect matching confidence,
malicious attacks targeting V2X positioning systems [19]–          to derive the final extrinsic parameters.
[21] pose another significant threat to their stability at urban      While classical spatial registration techniques [22] often rely
intersections. These challenges create a fundamental conflict      on iterative closest point algorithms with SVD for refinement
between the positioning demands and inherent instability of        or apply OT to point-level correspondences, these approaches
urban intersections, making this a critical bottleneck for the     struggle with the high outlier ratios, lack of initial alignment,
widespread adoption of V2X technologies.                           and scene-level complexity inherent in V2X scenarios. V2X-
   To address this issue, this paper introduces V2X-Reg++,         Reg++ differentiates itself by operating at the semantic level
an real-time global spatial registration method for multi-end      of detected objects, which is inherently suited to tackling
sensing systems that operates without external positioning sup-    the aforementioned V2X challenges. Methodologically, its key
port. By matching detected objects from vehicle and roadside       innovation is the synergistic framework of SVD and OT:
sensors, our method provides the real-time data alignment          SVD is not merely a final solver but an initial engine for
essential for reliable V2X applications.                           generating object-pair transformation hypotheses that underpin
   The core of our method is a strategic shift from direct         our oDist metric. This oDist then empowers a more robust and
cross-source sensor data processing to leveraging the spa-         meaningful OT stage for global object association, a distinct
tial topology of 3D detected objects. This perception-guided       departure from traditional point-based applications of these
approach dramatically reduces computational complexity and         techniques.
enhances robustness to noise and partial overlaps typical             The main advantages of V2X-Reg++ is its independence
in V2X environments. The method consists of two main               from initial external parameters without compromising its real-
components: a multi-end target association algorithm and an        time performance. Furthermore, this method leverages traffic
external parameter solution based on shared target . In the        participant information commonly present in traffic scenes,
multi-end target association component, we introduce a novel       enhancing its versatility. By processing information using only
Overall Distance (oDist) metric. This metric is informed by        perception data from target detection, it elegantly addresses the
initial SVD-based transformation hypotheses for candidate          challenge of high outlier ratios in cross-source point clouds
object pairs and quantifies scene-level spatial consistency.       arising from different sensor configurations. Compared to
This rich oDist measure then serves as the basis for an            other methods requiring complex data processing [13], V2X-
Reg++ has lower computational complexity and data transmis-          parameter estimation by detecting these corresponding fea-
sion costs, making it more suitable for practical applications.      tures. The advantage of such methods lies in the straightfor-
   The innovations of this paper are summarized as follows:          ward extraction and matching of local features for registration,
 1) An initial-value-free real-time spatial registration method      as well as controllable registration accuracy. However, in
    for vehicle-road multi-end sensing system is proposed,           large-scale dynamic settings such as urban intersections [8],
    particularly suited for environments like urban canyons          calibration targets may be occluded or cannot be left in place
    where positioning fails;                                         for an extended period. This inherent limitation underscores
 2) A new multi-end target association method is proposed,           their offline nature, making them unsuitable for the demands of
    which robustly establishes spatial associations across the       online registration and impractical for real-world deployment.
    scene even without positioning priors, and its core metric,
    Overall Distance (oDist), serves as a real-time indicator        B. Targetless Methods
    of external parameters quality among all participants;              Targetless approaches do not rely on additional calibration
 3) The effectiveness of the method is validated on both sim-        targets and can be further divided into motion-based and scene-
    ulated and real datasets, achieving real-time registration       based methods:
    of external parameters.                                             Motion-based methods [25], [26] utilize the trajectories
   In the preliminary conference paper [23], we proposed             of the sensor carrier or moving objects in the scene, together
a framework for initial-value-free multi-end LiDARs regis-           with geometric constraints, to estimate relative poses among
tration. This article builds upon our preliminary work by            sensors. These methods generally perform well in scenarios
introducing a more robust indicator of scene consistency, a          with stable and observable motion. However, if the scene is
novel weighting mechanism, an application example, and more          highly dynamic or if the motion trajectory is not controllable,
comprehensive experimental validation. Notably, we have re-          their accuracy and robustness can degrade significantly.
fined our core oDist metric (see Section IV-B2), superseding            Scene-based methods extract environmental features (e.g.,
our previous oIoU metric, yielding greater computational             edges, planes) or apply ICP-style iterative closest point al-
efficiency (see time comparisons in Table II and Table III)          gorithms [27]–[29] to register multi-end point clouds. These
and providing a more robust indicator of scene consistency           approaches do not require manual placement of calibration
(referencing the experiment in Fig. 8), further substantiated by     objects. Nevertheless, when initial values is inaccurate or in
new real-world application examples (Section VI). Addition-          highly dynamic environments, these algorithms may fail to
ally, V2X-Reg++ incorporates a novel weighted mechanism              register or may suffer from reduced accuracy [30], [31]. To ad-
into the final extrinsic parameter estimation stage (see Sec-        dress these challenges some researchers have introduced global
tion IV-C), which utilizes association confidences to signif-        registration [32]–[34] or graph-based optimization frameworks
icantly boost registration accuracy. The system’s capabilities       [35]–[38] to reduce dependence on initial values and overlap
and resilience are also more comprehensively demonstrated            areas. However, most of these algorithms still demand consid-
through expanded experimentation, including evaluations on           erable computational resources for real-time performance.
a new simulated V2X-Sim dataset (see Table II) and in-
depth comparative analysis (see Table III). Collectively, these
                                                                     C. Learning-Based Methods
enhancements deliver a method with demonstrably higher
accuracy and efficiency for real-time global registration in            In recent years, deep learning has made remarkable ad-
complex V2X environments.                                            vances in the extraction and matching of 3D features, enabling
                                                                     end-to-end registration [39], [40]. Compared with classical
                                                                     geometric methods, deep learning can learn feature represen-
                    II. RELATED WORK                                 tations that are more robust to noise and dynamic interference
   This paper focuses on the multi-end spatial registration          using large-scale data. Nonetheless, these approaches typically
method in urban intersection scenarios. The core objective of        require extensive labeled datasets or self-supervised signals,
this method is to solve the extrinsic parameters between differ-     and the training process is time-consuming. Furthermore,
ent sensors, i.e., to determine their relative pose relationships,   domain adaptation issues may arise when applying the trained
thereby achieving the spatial registration of multi-end sensing      models to new scenarios—such as different urban structures
system.                                                              or LiDARs with varying numbers of beams [41], [42].
   Some existing registration methods can be applied to the
scenario described in this paper. We categorize these methods        D. Multi-end Spatial Registration Methods for Urban Inter-
into three groups: Target-based, Targetless, and Learning-           section Scenarios
based methods.                                                          Urban intersections are challenging environments, often
                                                                     characterized by signal occlusions from surrounding build-
                                                                     ings, unreliable GPS signals, and a high density of traffic
A. Target-Based Methods
                                                                     participants with complex motion patterns. Multi-end spatial
   Target-based methods typically involve placing calibration        registration methods that rely on extrinsic priors or GPS
targets or boards with known dimensions and geometric fea-           positioning [6], [7], [11]–[14] are prone to failure in such
tures in the scene [24], and achieving high-precision extrinsic      settings. Meanwhile, classical ICP or feature-based registration
                                                                  C
                                                                    Bc . However, for simplicity, we only label cross-coordinate
                                                                  sets, such as C Be . Elements in Bs have been transformed into
                                                                  a unified coordinate system and will not be explicitly discussed
                                                                  further.
                                                                     Previous studies typically rely on positioning systems to
                                                                  obtain a prior extrinsic value, primarily focusing on solving
                                                                  the extrinsic optimization problem. In this work, we aim to
                                                                  eliminate the need for prior extrinsic value by fully exploiting
                                                                  relationships between Be and Bc .
                                                                     Then, a feature point cloud P is constructed from the object
                                                                  set Bs as follows:
                                                                                  h                        i⊤
                                                                             P = B̂s1 , B̂s2 , · · · , B̂sn , P ∈ R8n×3        (1)

                                                                  where B̂si represents the i-th target in B̂s , n denotes the number
                                                                  of co-viewing boxes.
                                                                     The ultimate goal is to obtain the optimal extrinsic parame-
                                                                  ters T̂ by minimizing the distance between the corresponding
    Fig. 3: Diagram of notations in problem formulation.          feature point clouds:
                                                                                (R̂, t̂) = arg min E(Pe , RPc + t)               (2)
                                                                                               R,t
methods can perform poorly due to limited overlap and exces-                                              
sive noise. Although deep learning algorithms offer potential                                   R̂      t̂
                                                                                           T̂ = T                                (3)
for tackling complex environments, they still face challenges                                   0       1
related to data collection, model generalization, and com-        where R̂ ∈ SO(3) is the rotation matrix, and t̂ ∈ R3 is the
putational efficiency in real-world applications. Additionally,   translation vector. Together, they form the extrinsic parameters
when registering sensors with different configurations (such as   T̂ ∈ SE(3) as shown in Eq.(3). Eq.(2) performs the rotation
beam counts, fields of view, resolutions, or scan frequencies),   and translation, transforming both point clouds into the same
discrepancies in data resolution and feature representation       coordinate system. The error metric, E(·), is computed using
must be carefully addressed [41]–[43].                            the L2 norm (Euclidean distance) between the feature point
   In summary, developing a spatial registration method for       clouds, as detailed in Eq.(14).
multi-end sensing systems in urban intersections that is real-
time and robust to initial pose values remains an open chal-                          IV. METHODOLOGY
lenge.
                                                                  A. Overview
                                                                     The proposed V2X-Reg++ framework addresses the multi-
            III. PROBLEM FORMULATION
                                                                  LiDARs registration problem in V2X scenarios from a sensor
   In this study, we construct a network of agents consisting     perspective, while fundamentally solving a cross-view cross-
of an Ego Agent and a Cooperative Agent (e.g., connected          source point cloud global registration problem at the data level.
vehicles or roadside units), denoted as A = {e, c}. We refer to   Instead of operating on dense, potentially noisy point clouds,
the objects detected by these agents as passive objects, which    which is computationally intensive and sensitive to outliers
do not participate in the information exchange. These objects     especially without initial alignment, our method operates at
serve as scene features, enriching the environmental descrip-     the semantic level of 3D detected objects. This stems from
tion. Each object in the set of passive objects B perceived in    an intuitive hypothesis: proper alignment of multi-view co-
the environment is represented as Bi = [pT , dT , θ]T ∈ R7 ,      visible objects implies valid extrinsic transformations between
where p ∈ R3 denotes the center position, θ ∈ [0, 2π)             perspectives. This is particularly relevant for V2X urban in-
represents the orientation, and d ∈ R3 represents the 3D          tersections, which typically provide sufficient co-visible traffic
dimensions. We can also express Bi as a vertex matrix B̂i .       participants (targets) to constrain the problem. Thus, we recast
Here, B̂i ∈ R3×8 represents the eight-vertex bounding box of      global registration primarily as a robust co-visible object
the passive object, where each column corresponds to a 3D co-     matching problem, addressed in two main stages.
ordinate of a vertex. For convenience, the two representations       First, shared targets across agents are associated through
Bi (emphasizing object aggregation) and B̂i (highlighting         spatial graph formed by targets, as illustrated in Fig. 4, where
matrix-based transformations) will be used interchangeably to     nodes denote objects and edges encode relative geometric
describe these passive objects in subsequent contexts.            constraints. This multi-end association mechanism is detailed
   From the perspectives of the Ego Agent and Cooperative         in Section IV-B.
Agent, the perceived object sets Be and Bc are obtained, and         Subsequently, we convert matched targets into weighted
their shared perceived objects are represented as Bs = Be ∩Bc .   feature point clouds Pe and Pc (Eq. 1), where confidence
   It is important to note that Be and Bc are located in their    scores Mi,j (Eq. 11) guide a robust weighted SVD algorithm
respective sensor coordinate systems, denoted as E Be and         to solve extrinsic parameters, as elaborated in Section IV-C.
   This dual-phase approach ensures registration accuracy by
prioritizing high-confidence matches while adaptively sup-
pressing noisy detections through graph-derived geometric
constraints, achieving real-time performance without prior
pose initialization.

B. Multi-End Object Association
   Unlike existing object association methods that predom-
inantly focus on short-term temporal continuity in object
tracking scenarios [44], our approach emphasizes scene-level
co-visible object matching under large perspective variations,
as illustrated in Algorithm 1. We propose a dual-stage com-
binatorial strategy integrating singular value decomposition           Fig. 4: The core idea of object association is to transform the
(SVD) and optimal transport (OT), with two main part: 1)               correspondence between targets into a correspondence of the
Scene-level correspondence mapping: Transform potential                spatial graph formed by the objects.
pairwise object matches (Bei , Bcj ) into coordinate-unified spa-
tial graphs (Fig. 4) through coordinate alignment (Eq. 4);
2) Optimal transport-based matching: Convert the spatial                                    H = UΣVT = B̂ei (B̂cj )T                         (6)
graph association problem into a first-order node transportation       Here, B̂ei and B̂cj are the vertex matrix representations of the
task, where the oDist metric quantifies transportation costs by        objects Bei and Bcj , respectively, as defined in Section III.
                                        ei
jointly evaluating match quantity τ C cj (Eq. 9) and precision         The matrices U and V are the left and right singular matrices
    ei
τ D cj (Eq. 10), enabling robust object correspondence through         obtained by applying Singular Value Decomposition (SVD) to
optimal transport theory.                                              H, following the method in [46].
                                                                          Crucially, this initial SVD is not intended to yield the
Algorithm 1 Multi-end Object Association
                                                                       final extrinsic parameters. Instead, its purpose is to provide
 1:   Input: Objects Be and Bc from Ego and Cooperative                a localized transformation hypothesis. This hypothesis allows
      Agents                                                           us to globally align the entire Cooperative Agent’s perceived
 2:   Output: Matched object pairs Bs                                  scene Bc with the Ego Agent’s scene Be if this specific object
 3:   Initialize affinity matrix M                                     pair (Bei , Bcj ) were a correct match. This potential localized
 4:   for Bei in Be do                                                 alignment is fundamental for the subsequent Spatial Similarity
 5:       for Bcj in Bc do                                             Assessment.
 6:            Calculate Fij (·) using Eq.(4)                             2) Spatial Similarity Assessment: After generating a hy-
               Ei,j c
 7:                B = Fij (Bc ) ▷ Transforming Bc based on            pothetical alignment from a candidate object pair (Bei , Bcj ),
      localized transformation hypothesis (Bei , Bcj )                 we introduce our novel oDist metric to assess its quality by
                             ei        ei
 8:            Calculate τ Dcj and τ C cj using Eq.(9) and Eq.(10)     quantifying the resulting scene-level spatial consistency.
                      ei
 9:            if τ Dcj < τ1 then                                         To compute oDist, let Ei,j Bc = Fij (Bc ) denote the trans-
10:                Update Mi,j = τ C cj
                                       ei                              formation of all objects Bc from the Cooperative Agent to
11:            else                                                    the Ego Agent’s coordinate system, based on the alignment
12:                Update Mi,j = 0                                     hypothesis derived from the specific pair (Bei , Bcj ). Within this
13:            end if                                                  hypothetically aligned scene, we define a valid matching pair
14:       end for                                                      set τ Vceji . This set comprises pairs of objects—one from Be
15:   end for                                                          and one from the transformed Ei,j Bc —that satisfy a distance
16:   Get assignment matrix X by solving Eq.(12) using [45]            threshold τ . The specific mathematical definition is given as:
17:   Extract matched pairs BS from Be and Bc using X                             ei
18:   Return BS                                                                τ Vcj   = {(Bei′ , Ei,j Bcj′ ) | d(Bei′ , Ei,j Bcj′ ) ≤ τ }   (7)
                                                                        d(Bei′ , Ei,j Bcj′ ) = α∥pei′ − Ei,j pcj′ ∥ + β∥B̂ei′ − Ei,j B̂cj′ ∥ (8)
   1) Correspondence Mapping: Define Bei and Bcj as the i-
th and j-th perception objects in Be and Bc , respectively. The           Here, τ is a threshold designed to encompass potential
first step in our association pipeline is to generate hypothetical     matches, which can be empirically adjusted between 0 and
alignments. For every potential pair of objects (Bei , Bcj ), we       3 depending on the level of noise present in the scene.
compute a relative transformation Fij (·) :                            pei′ represents the spatial center of the object Bei′ , and B̂ei′
                                                                       represents the vertex matrix of the i′ -th perception object. α
                   Fij (·) = Rij · +pei − Rij pcj                (4)   and β are weight factors that adjust the contribution of the
where · is a placeholder for any input point set,   pei
                                                     and   pcj
                                                             are       location center and vertex distance of the detection frame to
center position of Be and Bc respectively. Rij is calculated as:       the index.
                                                                          The oDist itself consists of two key components, both
               Rij = Udiag(1, 1, det(UVT ))VT                    (5)   calculated from the valid matching set τ Vceji :
                                ei
   A confidence metric, τ C cj , which measures how many other
object pairs become well-aligned (i.e., fall within τ Vceji ) under
the current transformation hypothesis. This directly indicates
the degree of matching for the candidate pair (Bei , Bcj ).

                            ei          ei
                        τ C cj =card(
                               ˙     τ Vcj )                    (9)                      (a) Uniform Point Cloud Registration

                                ei
   A distance metric, τ Dcj , which quantifies how close the
geometric correspondence is for these well-aligned pairs by
calculating their average distance. This provides an indication
of the potential matching error for the candidate pair (Bei , Bcj ).

                        P                                                               (b) Weighted Point Cloud Registration
                 ei      (Bm ,Bn )∈τ Vcji
                                            e   d(Bm , Bn )
             τ D cj =
                    ˙                                          (10)
                                     card(τ Vceji )

   In these equations, card(·) denotes the number of elements          Fig. 5: Comparative illustration of uniform and weighted point
in a set, and d(·, ·) is defined in Eq.(8).                            cloud registration. In weighted point cloud registration, points
                                                      ei               with lower confidence have a larger range of spatial tolerant
   Therefore, the oDist, through its confidence τ C cj and dis-
          ei                                                           areas, allowing points with higher confidence to predominantly
tance τ Dcj components, provides a rich, context-aware score.          influence the registration outcome.
This score reflects the global plausibility of the initial local
match (Bei , Bcj ) by considering its impact on the entire scene’s
consistency, moving significantly beyond simple pairwise ob-              Based on the above, we define the shared target matching
ject similarity. This comprehensive assessment is crucial for          task as follows:
robustly identifying true correspondences in the subsequent
                                                                                              n X
                                                                                                m
object matching stage.                                                                        X
                                                                                     argmin              −Xi,j Mi,j       s.t.   X∈Π
   3) Object Matching: The robust oDist scores form the basis                           X     i=1 j=1
of our object matching stage. Then, we construct an affinity                                    n1 ×n2
                                                                        Π = {X | X ∈ {0,1}          , X1n2 ≤ 1n1 , XT 1n1 ≤ 1n2 }
matrix M between the perceived objects from the Ego Agent
                                                                                                                                      (12)
Be and the Cooperative Agent Bc . The elements Mi,j of this
                                                                       where n1 and n2 represent the number of elements in Be and
matrix quantify the likelihood of Bei and Bcj being a correct
                                                                       Bc respectively, and X is the assignment matrix. If Xi,j =
match, based on the oDist components:
                                                                       1, it indicates that Bei and Bcj are a matching pair, with the
                            (   ei               ei
                                                                       matching confidence given by Mi,j . This constitutes a typical
                            τ C cj       if τ Dcj < τ1 ,               linear programming task, solved using [45]. The final set of
                Mi,j =                                         (11)
                            0            otherwise.                    shared objects Bs can be represented as:
                                                                                       n                                            o
                                                                                                                   Xi,j =1
where τ1 represents a derived secondary filtering threshold,                     Bs = (Bei , Bcj , Mi,j ) | i=1,...,n 1 ;j=1,...,n2
                                                                                                                                      (13)
empirically adjustable between 0 and 2 based on the scene’s
                                              ei
noise level. This threshold applies to τ Dcj , which is the
average distance of all valid matching pairs in τ Vceji . This is      C. Extrinsic Parameter Estimation
subtly different from τ in Eq.(7), which refers to the distance           Once the set of shared object pairs Bs is established
between single potential matching pairs. The threshold τ has           with associated matching confidences Mi,j , we proceed to
a slightly larger range to include as many potential matching          estimate the final extrinsic parameters. The corners of these
pairs as possible, whereas τ1 is used to minimize the impact           matched 3D detection boxes are used to form feature point
of local spatial graph similarities on the calculation of valid        clouds Pe (for the Ego Agent) and Pc (for the Cooperative
matching pair similarity. This affinity matrix M effectively           Agent), as per Eq.(1). Crucially, these are not uniform point
captures the scene-level consistency for each potential object         clouds; each point pair derived from an object pair inherits the
pair, as evaluated by oDist.                                           matching confidence Mi,j established in the object association
   On this basis, we formulate an Optimal Transport (OT)               phase, effectively making Pe and Pc weighted feature point
problem to minimize the transformation cost from Bc to Be              clouds. As illustrated in Fig. 5, this weighting provides greater
based on oDist. Considering constraints in real scenarios, we          tolerance for points from less certain object matches, allowing
set two restrictions: 1) Each target in Bc can be matched with         the algorithm to prioritize the alignment of points from high-
at most one counterpart in Be ; 2) Not all targets can find            confidence object pairs.
matches due to differences in perception conditions such as               The weighted point cloud registration problem can be for-
field of view and occlusions.                                          mulated as follows:
                                                                               SuccessRate@λ: The SuccessRate@λ is defined as the
                             X                                              proportion of registration trials in a total sample set S =
   (R̂, t̂) = arg min                      wi ∥Rpei   +t−   pci ∥2   (14)   {1, 2, ..., N } for which the achieved RTE is below a predefined
                  R,t
                        pei ∈Pe ,pci ∈Pc
                                                                            threshold λ. This metric reflects the method’s reliability in
where pei ∈ R3 and pci ∈ R3 represent matched points                        achieving a specified level of accuracy. The SuccessRate@λ
in Pe and Pc , respectively. The weight wi is the matching                  is mathematically expressed as:
confidence Mi,j from Eq.(11). R̂ ∈ SO(3) and t̂ ∈ R3                                            λ
                                                                                               Svalid = {i ∈ S|RTEi < λ}                     (20)
represent the rotation matrix and translation vector of the target
extrinsic parameters T̂ in Eq.(3).                                                                                        λ
                                                                                                                       |Svalid |
   To solve this problem, we construct the weighted cross-                                     SuccessRate@λ =                               (21)
                                                                                                                         |S|
covariance matrix as follows:
                                                                              Since SuccessRate@λ only measures success frequency,
                    n
                    X                                                       we introduce the mean Relative Rotation Error (mRRE@λ)
              H=           wi (pei − pe )(pci − pc )T                (15)   and mean Relative Translation Error (mRTE@λ) to quantify
                     i=1
                                                                            the accuracy of these successful trials. These metrics are
               Pn        e
                                           Pm        c
                i=1 wi pi                    i=1 wi pi
                                                                            calculated by averaging the RRE and RTE values exclusively
           pe = P n        ,           pc = P m                      (16)   over the subset of trials deemed successful by the threshold
                  i=1 wi                      i=1 wi
                                                                            λ. The metrics are then defined as:
where pe and pc are the weighted centers of Pe and Pc ,                                                   P
respectively.                                                                                                   λ
                                                                                                            i∈Svalid RREi
                                                                                            mRRE@λ =            λ
                                                                                                                                    (22)
   Subsequently, singular value decomposition (SVD) [46] is                                                   |Svalid |
applied to H to obtain the left singular matrix U and right                                               P
singular matrix V, and these are used in Eq.(5) to determine                                                    λ
                                                                                                            i∈Svalid RTEi
                                                                                             mRTE@λ =           λ
                                                                                                                                    (23)
the rotation matrix R̂. The translation vector t̂ can be obtained                                             |Svalid |
by:
                                                                               The choice of the threshold λ is critical and is guided by the
                         t̂ = Pe − R̂Pc                      (17)
                                                                            operational requirements of Vehicle-to-Everything (V2X) ap-
  The final extrinsic parameters T̂ can be obtained using                   plications, particularly for downstream tasks like cooperative
Eq.(3).                                                                     perception and data fusion [6], [7], [11], [14], [47]. Drawing
                                                                            from existing literature and V2X system considerations (e.g.,
                        V. EXPERIMENT                                       [14] suggests λ = 2m, and [47] suggests λ = 3m), and with
                                                                            recommendations from some perception algorithms [6], [7],
  This section details the experimental setup used to evaluate              [11], an RTE threshold λ in the range of 1 to 3 meters is
our method, including the metrics, datasets, and validation                 often considered an acceptable upper bound for maintaining
procedures.                                                                 the requisite accuracy of these downstream tasks.

A. Evaluation Metrics                                                       B. Dataset
   To quantitatively evaluate our registration method, we first
                                                                               In this study, we used the simulated dataset V2X-Sim [48]
define the fundamental error metrics for a single trial: the
                                                                            and the real-world dataset DAIR-V2X [49] for experimental
Relative Rotation Error (RRE) and the Relative Translation
                                                                            validation. Both datasets contain extensive data collected from
Error (RTE). Based on these, we establish three key per-
                                                                            Vehicle-Everything Cooperative Autonomous Driving (VX-
formance indicators that will be used for our final analysis:
                                                                            CAD) scenarios, including LiDAR data from vehicles and
the Success Rate at a given threshold (SuccessRate@λ),
                                                                            infrastructure as well as their 3D bounding box annotations
the Mean Relative Rotation Error (mRRE@λ), and the Mean
                                                                            and ground truth extrinsic parameters. The specifications of
Relative Translation Error (mRTE@λ).
                                                                            LiDAR Equipment are presented in Table I.
   Relative Rotation Error (RRE): Measures the accuracy of
the rotational part of the registration result, i.e., the angular                  TABLE I: Specifications of LiDAR Equipment
difference between the estimated rotation matrix Re and the
true rotation matrix Rt .                                                                Parameter                DAIR-V2X            V2X-Sim
                                                                                                                   R      V          R       V
                               tr(R−1                                              LiDAR Points (lines)          300     40          32      32
                                                
                                     t Re ) − 1
              RRE = arccos                                   (18)               Horizontal Field of View (°)      100    360        360     360
                                       2                                         Max Detection Range (m)          280    200         70      70
   Relative Translation Error (RTE): Assesses the accuracy                           Volume (frames)             3737   3737        1000   1000
of the translation vector in the registration result, i.e., the                              Note: R = Roadside, V = Vehicle-side
distance difference between the estimated translation vector
te and the true translation vector tt .                                       One fundamental assumption for the effectiveness of the
                                                                            evaluation metrics discussed in the previous section is that
                        RTE = ||t−1
                                 t − te ||2                          (19)   the ground truth extrinsic parameters are sufficiently accurate.
To justify this assumption, we analyze the processing work-
flows of the two datasets. V2X-Sim [48] utilizes the SUMO
[50] and CARLA [51] simulation platforms, which model
the interaction between multiple vehicles and road-side units
(RSUs) and the data acquisition process of their sensors. In
this simulated environment, the data collection is considered
to be free of delays and annotation errors, making the ground
truth extrinsic parameters theoretically error-free. On the other
hand, DAIR-V2X [49] is based on real-world data collected
from actual scenarios. After filtering, cleaning, localization
system transformation, and pose refinement, relatively accurate
ground truth extrinsic parameters are obtained. Although some
errors still exist, they are generally considered to be within an
acceptable range.

C. Validation of Method Effectiveness                               Fig. 6: Heatmap Analysis of three performance metrics under
  In this section, we perform comprehensive experimental            varying noise levels applied to ground truth bounding boxes
validations of our method using the datasets and metrics            in the V2X-Sim Dataset. Despite increasing errors with higher
outlined above. This includes verifying the theoretical validity    input noise, the method shows strong noise resilience.
and noise sensitivity of the method on simulated datasets and
evaluating its performance under real noise conditions on real-
world datasets. All experiments were conducted on a device          (mean µ1 = 0 m, with standard deviation from 0 to 2.0m)
with an Intel i7-9750H CPU.                                         and von Mises noise into its orientation (mean µ2 = 0◦ ,
  1) Validity Verification on Simulated Dataset: We first           with standard deviation from 0° to 25°). This range effectively
validate the geometrical feasibility of our method under these      covers performance from typical error levels to more severe
perfect conditions. The specific results are shown in Table II.     failure cases.
                                                                       As shown in Fig. 6, while the method is resilient to
          TABLE II: Comparative Results on V2X-Sim.                 individual error dimensions, a combination of translation and
                                                                    rotation noise leads to a more rapid decline in effective-
                   mRRE(°) mRTE(m)        SuccessRate(%)
     Method                                                Time (s) ness. Encouragingly, our method still demonstrates a strong
                     @3°        @3m       @1m @2m                   corrective capability even under high noise. Specifically, the
    FGR [34]         0.69       0.16      78.64 95.15        0.92   maximum mRTE@10 and mRRE@10 were controlled at 1.8m
   Quartro [52]      0.17       0.18      96.40 98.20        0.83   and 3.5◦ , respectively. This confirms the method’s robustness
  Teaser++ [32]      0.77       0.17      76.70 94.17        0.91
  V2X-Reg [23]       0.06       0.03      93.26 95.48        0.37   across a realistic spectrum of perception noise.
   V2X-Reg++         0.01       0.01      96.80 98.31        0.13      3) Validity Verification on Real-World Dataset: To assess
                                                                    our method’s practical efficacy, we performed experiments on
   We find that under conditions of perfect detection and the DAIR-V2X dataset [49]. Its varied LiDAR configurations
perfect transmission, V2X-Reg++ achieves near-zero rotation (Table I) and sparse point cloud overlaps (Fig. 7) pose sub-
and translation deviations quickly, proving the geometrical stantial registration challenges.
correctness of the perception-based approach. It is also ob- Performance and Accuracy.
served that even in ideal conditions, the success rate of the          Comparison with methods requiring initial values: The
method is not 100%. This is due to the spatial information upper part of Table III contextualizes our results against
between perception objects occasionally having similarities, methods reliant on initial poses. Notably, real-world V2X point
which can affect our method. However, these coincidental clouds suffer from heterogeneity and cross-view discrepancies.
spatial similarities are rare, and the likelihood can be reduced Coupled with potential inaccuracies in DAIR-V2X ground-
by increasing the number of perception objects.                     truth extrinsics, even initial-value-based methods [56], [58]
   2) Sensitivity to Noise: To evaluate our method’s robustness under ideal conditions (N oise = 0m and 0◦ ) exhibit an
against realistic perception errors, we conducted a noise injec- approximate 0.5m and 0.5◦ error. This can be seen as an
tion experiment on the V2X-Sim dataset [48]. We grounded accuracy upper bound for this dataset. Introducing equivalent
our noise model in the performance of a widely-used 3D rotational and translational noise to these methods rapidly
detector, PointPillars [53], on the nuScenes benchmark [54]. degrades their accuracy, rendering them nearly unusable at
On this benchmark, PointPillars achieves a mean Average 2m and 2◦ noise. V2X-Reg++, however, effectively mitigates
Translation Error (mATE) of 0.32m and a mean Average initial pose deviations. Its performance remains comparable
Orientation Error (mAOE) of 0.28rad (≈ 16◦ ).                       to the dataset accuracy upper bound (e.g., the mRTE@1m)
   Recognizing that these are average metrics and that individ- and demonstrates stability against increasing initial errors,
ual detections, especially in challenging scenarios, can exhibit underscoring its practical value.
larger deviations, our experiment was designed to span a wider         Comparison with initial-value-free methods: The lower
spectrum. We injected Gaussian noise into the object’s position part of Table III shows V2X-Reg++ significantly surpass-
TABLE III: Comparative Results on the DAIR-V2X Dataset. For the methods that require initial pose values, we add noise of equal magnitude
to the rotational and translational dimensions to simulate different levels and sources of noise in real-world scenarios. Lower values are better
for mRRE and mRTE ( ↓), and higher values are better for SuccessRate (↑). Subscripts GT, PP, and SC denote ground-truth boxes, PointPillars
[53] detector boxes, and SECOND [57] detector boxes, respectively. The superscript k signifies the use of top-k dimension-sorted boxes, while
∞ indicates use all boxes provided. The best and second-best results are highlighted in each section.

           Noise                                        mRRE (°) ↓                     mRTE (m) ↓                 SuccessRate (%) ↑
Init                        Method                                                                                                           Time (s) ↓
         (m & ◦ )                               @1◦        @2◦       @3◦       @1m        @2m       @3m        @1m       @2m        @3m
             0                                  0.65      0.98       1.07       0.42       0.54      0.58    47.52      89.55     96.01         2.91
             1              ICP [55]            0.80      1.36       1.72      0.66       1.31      1.62      0.86      37.93     80.50         2.92
             2                                  0.00      1.48       2.11      0.00       1.33      2.03      0.00       3.66     19.94         2.86
             0                                   0.52      0.80       0.88      0.42       0.54      0.57      59.59     90.41     96.12        1.35
             1             PICP [56]            0.74      1.31       1.67      0.75       1.32      1.63       2.91     42.78     87.93         1.76
✓            2                                  0.80      1.40       2.11      0.53       1.45      2.10       0.22      2.69     21.12         1.70
             0                                  0.63       0.89       0.99     0.54       0.78      0.89      54.20      88.69     97.63        0.46
             1             VIPS [14]            0.66      1.04       1.24      0.54       0.82      1.02     18.53      39.01     47.74         0.44
             2                                   0.58     1.17       1.56       0.48      0.96      1.39      2.37       7.87     13.15         0.47
             0                                  0.61      0.97       1.21      0.53       0.80      1.06     17.11      23.04     26.49         0.35
             1            CBM [12] †            0.71      0.94       1.14      0.61        0.74     1.00      9.91      15.63     16.49         0.36
             2                                  0.69      1.09       1.38      0.58       0.76      1.06      6.03      12.28     16.81         0.35
             -             FGR [34]             0.71      1.15       1.47      0.70       1.13      1.45     14.76      31.57     35.34        22.73
             -            Quartro [52]           0.62     1.22       1.46      0.65       1.19      1.51     12.07      30.50     45.04        21.58
             -           Teaser++ [32]          0.69      1.13       1.47      0.66       1.09      1.44     14.33      29.74     34.81        22.43
  ×          -           V2X-Reg [23]           0.66       1.03       1.25     0.54       0.91      1.18     25.54      55.93     72.31         0.21
             -          V2X-Reg++GT ∞            0.62      1.01      1.26       0.49       0.83     1.07     22.88      48.03     61.49         0.46
             -          V2X-Reg++GT 25           0.63      1.01       1.23      0.52       0.85      1.05     32.27      67.59     82.93        0.12
             -          V2X-Reg++GT 15          0.65      1.05       1.30      0.54       0.87      1.10      26.79      61.17     78.75        0.09
             -          V2X-Reg++GT 10          0.66      1.11       1.36      0.57       0.92      1.15     20.02      54.86     71.98         0.04
             -          V2X-Reg++PP 15          0.66      1.06       1.29      0.55       0.86      1.07     24.91      56.62     70.94           -
             -          V2X-Reg++SC 15          0.65      1.05       1.29      0.54       0.86       1.06    25.15      56.89     71.23           -
             -      V2X-Reg++GT 25 (hSVD)‡      0.71      1.13       1.35      0.62       0.98      1.25     21.82      60.43     74.92         0.12
             -      V2X-Reg++GT 25 (mSVD)‡      0.67      1.08       1.31      0.56       0.94      1.19     25.22      63.58     80.22         0.12
  †
       : For CBM [12], our reimplementation (due to partial code availability) achieves comparable accuracy but significantly lower success rates under
       SuccessRate@λ. We will make it open-source in our codebase.
  ‡
       : V2X-Reg++ entries without parentheses (e.g., V2X-Reg++GT 25 ) use the proposed Weighted SVD (wSVD) by default. Comparisons between wSVD, mSVD,
       and hSVD strategies (Section V-E2) validate wSVD’s superior robustness.



ing other initial-value-free techniques. It achieves a high                    We posit this is due to inherent spatial deviations in V2X
SuccessRate@2m of 56.89% and low mRTE@2m of 0.86m,                             scenarios, which dilute the influence of perceptual errors
even with real-world detector outputs from SECOND [57]                         on extrinsic parameters in successful registrations. Stronger
and PointPillars [53]. This suggests that employing detection                  perceptual errors tend to cause registration failure and are
boxes as an intermediate representation for global registration                thus excluded from accuracy statistics. The overall decrease
of cross-source point clouds with large FoV differences can                    in SuccessRate@λ is acceptable, reflecting our method’s
match the accuracy of point-based or feature-based methods.                    robustness to perceptual noise by emphasizing high-confidence
Crucially, it also enables real-time processing and substantially              matches. In this context, perceptual uncertainty’s impact can
reduces transmission bandwidth, making it well-suited for                      be mitigated by pre-processing, such as expanding detection
V2X challenges.                                                                ranges and filtering low-confidence boxes, enhancing practical
   An interesting observation is that while point/feature-based                viability.
methods [56], [58] achieve higher accuracy with initial values                 Detection Box Quantity Effects. Our method’s reliance on
than detection-box-based methods [12], [14] , while ours are                   detection boxes means their quantity is a key factor, alongside
more accurate without initial values, outperforming methods                    perceptual uncertainty. DAIR-V2X frames exhibit wide varia-
like [32], [34], [52], indicating that the high-level semantic                 tion in box counts (from a few to tens). Our SVD-based extrin-
information provided by detection boxes is better suited for                   sic search (Section IV-B1) naturally favors larger-dimension
coarse registration tasks.                                                     boxes due to greater spatial distinctiveness and potentially
Impact of Detection Uncertainty. To quantify the impact of                     higher annotation/detection accuracy. We thus sorted boxes
real-world perception noise, we used detections from PointPil-                 by volume and experimented with top-K subsets. Table III
lars (PP) [53] and SECOND (SC) [57] instead of ground-truth                    indicates that performance generally improves with more
(GT) boxes. While perception errors cause slight performance                   boxes. However, using all boxes (V2X-Reg++GT ∞ ) degrades
degradation, V2X-Reg++ remains robust overall. The impact                      performance. Fig. 7(c) illustrates this with small traffic cones
of detector errors on overall registration accuracy in real                    from DAIR-V2X, which are challenging to annotate/detect ac-
scenes is considerably smaller than in simulations (Fig. 6),                   curately. Their low spatial distinctiveness leads to larger SVD-
sometimes even yielding better mRTE@3m values. The main                        derived extrinsic errors, impacting overall performance. Thus,
adverse effect of detection errors is on SuccessRate@λ.                        pre-filtering input boxes significantly boosts performance and
  (a) Ideal Scenario from V2X-Sim   (b) Ideal Scenario from DAIR-V2X       (c) Challenging Scenario           (d) Complex Scenario

Fig. 7: Comparative Registration Results Across Diverse Scenarios: (a, d) Accurate alignment of multi-end perception objects.
(b) Registration errors due to small-sized perception objects. (d) Suboptimal registration results due to inherent perception
errors.


reduces computation time. Comparing V2X-Reg++GT 15 , V2X-              between multi-end perception targets during the monitoring of
Reg++PP 15 , and V2X-Reg++GT 10 reveals that box quantity has          external parameters. This causes an occasional rise in metrics
a greater impact on the method’s performance than individual           when Bei moves away from Bcj and closer to Bcj+1 . In contrast
box uncertainty. This aligns with our method’s focus on                to the oIoU metric, the oDist metric proposed in this paper
information derived from the set of boxes, making set size             extends the valid association phase from merely considering
(quantity) more influential than individual box characteristics        the IoU of two targets greater than zero to a set threshold
(uncertainty).                                                         distance determination, akin to the improvements suggested
Runtime Analysis. Preliminary multi-threaded Python opti-              by softNMS [59] on the classical NMS algorithm.
mizations on an Intel i7-9750H CPU @2.6GHz yield a runtime
of 0.09s for V2X-Reg++GT 15 , which is significantly below the
0.35s requirement in [13]. This result is primarily intended
for relative comparison with existing methods, highlighting            E. Ablation Experiments
the algorithm’s real-time capability. Furthermore, the approach
                                                                          In this section, we aim to validate the efficiency of the
exhibits substantial parallelization potential. Specifically, the
                                                                       object association module introduced in Section IV-B and the
computational bottleneck arises from the O(N 2 ) oDist cal-
                                                                       external parameter optimization module discussed in Section
culations for potential pairs; however, these operations are
                                                                       IV-C through a series of ablation experiments.
mutually independent and thus amenable to parallelization.
Additionally, the underlying matrix operations are well-suited            1) Comparison of Object Association Strategies: To vali-
for GPU acceleration. While the runtime upper bound oc-                date the superiority of the target association module strategy
curs with V2X-Reg++GT ∞ (dozens of boxes), we note that                proposed in this paper, we conducted a comparative analysis
such configurations may reduce accuracy, as discussed earlier.         with the angle-based and length-based association strategies
Therefore, practical deployment requires a trade-off between           defined in [14], as well as the oIoU metric association strategy
selected box quantity and runtime efficiency.                          defined in [23]. As shown in Fig. 9, the proposed associ-
                                                                       ation strategy (Strategy 1) and the oIoU metric association
                                                                       strategy (Strategy 2) both achieved high target association
D. Effectiveness Test of Overall Distance                              rates. However, the oDist metric strategy excelled, achieving
   The multi-end spatial registration method proposed in this          superior final success rates for both λ = 1 and λ = 2, thus
paper centers around the oDist metric for achieving tar-               confirming its robustness. The angle-based (Strategy 3) and
get association. Compared to oIoU metric proposed in our               length-based (Strategy 4) association strategies defined in [14],
previous method V2X-Reg [23], the oDist exhibits superior              originally intended for scenarios with initial external parameter
performance and can better assess associations with objects            values, did not perform as well overall. Nonetheless, they
that are further away. To validate the trends of the oDist metric      demonstrated some applicability, especially the length-based
under various noise conditions, we designed experiments on             association strategy 4, which showed certain performance
DAIR-V2X [49].                                                         characteristics without initial external values.
   As shown in Fig. 8, compared to the oIoU metric, the                   2) Comparison of Strategies for External Parameter So-
oDist proposed in this paper demonstrates a smoother curve of          lution: To validate the superiority of the external parameter
change with respect to different deviations in the external pa-        optimization module discussed in Section IV-C, we compared
rameters. This implies that it is less susceptible to falling into     three strategies: the Weighted SVD (wSVD) method proposed
local mathematical optima, thus providing better monitoring            in this paper, the Mean SVD (mSVD) method, and the SVD
performance for external parameter alignment.                          method based on the highest confidence detection box (hSVD).
   It is worth explaining that under continuously increasing           As shown at the bottom of Table. III, we observed that the
biases, most perception-object-based metrics exhibit this type         Weighted SVD method proposed in this paper demonstrated
of local rise in values, due to the lack of confirmed associations     enhanced robustness.
   (a) Comparison of metrics along the x-axis   (b) Comparison of metrics along the y-axis   (c) Comparison of metrics along the yaw dimension

Fig. 8: Verification of the indicator effects of the oDist metric proposed in this paper and the oIoU metric proposed in [23]
on the initial external parameters with added noise in different directions on DAIR-V2X. It is observed that the oDist metric
proposed by this method displays smoother curves, indicating better performance of oDist in monitoring the alignment level
of external parameters.


                                                                        optimize the extrinsic parameters, as detailed in Algorithm
                                                                        2. This mechanism creates a robust synergy with positioning
                                                                        systems. In environments like urban canyons where position-
                                                                        ing signals are unreliable, V2X-Reg++ acts as a perception-
                                                                        level fail-safe, providing the precise and continuous alignment
                                                                        required for all V2X applications.

                                                                                              VII. DISCUSSION
                                                                        A. Multi-End Spatial Registration Tasks and Localization
                                                                        Tasks
                                                                           While localization and multi-end spatial registration are
                                                                        related, as both express spatial relationships, they are funda-
                                                                        mentally distinct tasks in terms of their objectives, target ob-
                                                                        jects, and role in the autonomous driving pipeline. Specifically,
Fig. 9: Violin plot comparing the effects of different object           localization typically aims to determine a vehicle’s absolute
association strategies. The Matches Rate refers to the ratio            position within a global coordinate system, focusing on the
of the number of correctly matched objects to the number of             vehicle’s body frame. In contrast, registration, the focus of
ground truth matched objects annotated in the scene. Strategy           this paper, seeks to resolve the relative pose transformation
1 is the oDist metric proposed in this paper, Strategy 2 is             between different sensor coordinate systems. This places as
the oIoU metric proposed by [23], and Strategies 3 and 4 are            a foundational upstream task, responsible for enabling the
the angular and length similarity metrics commonly used in              accurate spatial alignment of sensor data, whereas localization
papers such as [14]. The effectiveness of the target association        is a downstream application that often relies on this pre-
strategy adopted in this paper is evident.                              aligned data. Furthermore, the real-time nature of our method
                                                                        should not be misconstrued as a characteristic exclusive to
                                                                        localization. While classic was often a static, offline process,
                                                                        modern targetless methods are increasingly evolving towards
                  VI. P RACTICAL U SE C ASES
                                                                        continuous, real-time operation to meet the demands of V2X
   To demonstrate the practical value of V2X-Reg++, this sec-           systems. Therefore, based on its focus on relative sensor poses
tion outlines a typical scenario illustrating its role in enhancing     and its role as an upstream enabler for data fusion, our method
the robustness of V2X systems from initial deployment, partic-          firmly resides within the domain of sensor registration.
ularly focusing on boot-up safety registration and its synergy
with positioning systems.                                               B. Trends in Multi-End Spatial Registration
   V2X-Reg++ can significantly enhance the safety and reli-                The development of multi-sensor spatial registration tech-
ability of V2X systems during initial deployment and oper-              nology, as the basis for multi-sensor data fusion, largely
ation. Through the introduced oDist metric, the system can              depends on the demands of subsequent perception fusion
continuously monitor the spatial alignment quality of multi-            tasks. Similarly, the trends in multi-end spatial registration
end sensing system. If a registration deviation is detected at          tasks are influenced by multi-end perception fusion tasks.
system boot-up due to poor initial extrinsic parameters (e.g., a        One trend in the latter is to achieve better perception effects
bad oDist value), or if misalignment occurs during operation            under inaccurate external parameters [6], [15], [60], [61]. From
due to parameter drift, V2X-Reg++ will be triggered to quickly          another perspective, this integrates the registration process
Algorithm 2 V2X-Reg++ Registration & Monitoring                    ods that rely on positional priors, enabling effective registra-
 1:   Input:                                                       tion of multi-end sensing systems in environments with unsta-
 2:     P1 , P2 : Detection boxes from two sensors                 ble positioning signals, such as urban canyons. The method
 3:     θb : Boot threshold for initial registration               combines a two-stage SVD algorithms and optimal transport
 4:     θm : Monitor threshold for runtime checks                  theory to effectively solve the problem of data consistency
 5:     Rmax : Max retry attempts                                  among multi-end sensors. Additionally, our proposed Overall
 6:   Output: Tcur : Current extrinsics                            Distance (oDist) metric offers a reliable method for monitoring
 7:                                                                the quality of the spatial alignment in real-time.
 8: if stored Tstor exists then                                       Extensive experiments on the V2X-Sim and DAIR-V2X
 9:      Tcur ← Tstor                                              datasets demonstrate the effectiveness and robustness of V2X-
10: else                                                           Reg++. It provides stable, high-precision registration where
11:      Tcur ← null                                               initial-value-dependent methods fail due to noise, and it out-
12: end if                                                         performs other initial-value-free approaches in both success
13:                                                                rate and computational efficiency. The method is also ro-
14: if Tcur = null or oDist(P1 , P2 , Tcur ) > θb then             bust to perception noise, maintaining low registration errors
15:     Tcur ← Registration(P1 , P2 , θb , Rmax )                  even when using noisy detections from real-world detectors,
16:     if Tcur = fail then trigger alert                          as shown in our DAIR-V2X experiments. Its computational
17:     end if                                                     efficiency, with runtimes as low as 0.1 seconds, fully satisfies
18: end if                                                         real-time demands. However, its primary limitation is a direct
19:                                                                dependency on the quantity and quality of co-visible detection
20:   while system running do                                      boxes. As performance can degrade with either too few objects
21:      Acquire new P1 , P2                                       or an excess of low-quality detections, this paper also proposes
22:      if oDist(P1 , P2 , Tcur ) > θm then                       some pre-processing strategies in the experimental analysis
23:          Tnew ← Registration(P1 , P2 , θm , Rmax )             section to mitigate these effects.
24:          if Tnew ̸= fail then                                     Future research will continue to explore the application
25:              Tcur ← Tnew ; store Tcur                          potential of V2X-Reg++ in broader urban environments. This
26:          else                                                  includes integrating other sensor types, leveraging static infor-
27:              degrade operation                                 mation from maps to extend its applicability to scenes with
28:          end if                                                fewer dynamic objects, and scaling the algorithm for multi-
29:      end if                                                    agent systems. Expanding the current pairwise framework to
30:      apply Tcur                                                robustly handle multiple agents in multi-frame asynchronous
31:   end while                                                    scenarios is a key direction, potentially by using our pairwise
32:                                                                results as edges in a global pose-graph optimization network.
33:   function R EGISTRATION(P1 , P2 , θ, R)                       However, this introduces communication bottlenecks, and
34:      r←0                                                       future work will investigate co-optimizing registration with
35:      while r < R do                                            communication strategies. On the performance front, given the
36:          T ← V2X-Reg++(P1 , P2 )                               algorithm’s reliance on matrix operations, significant speed-
37:          if oDist(P1 , P2 , T) ≤ θ then                        ups are anticipated from GPU-accelerated implementations.
38:              return T                             ▷ Success    Additionally, as autonomous driving technologies advance,
39:          end if                                                integrating V2X-Reg++ more deeply with the full autonomous
40:          r ←r+1                                                driving stack will be a critical area for subsequent research.
41:      end while                                                 By providing a robust and real-time solution for initial-
42:      return fail                       ▷ All attempts failed   value-free sensor registration, we believe V2X-Reg++ offers
43:   end function                                                 a foundational technology essential for advancing cooperative
                                                                   perception and, consequently, improving the safety and intel-
                                                                   ligence of future automated urban traffic systems.
into the perception task, no longer treating registration as an
independent output but as a dynamically adjusted intermediate
                                                                                                R EFERENCES
quantity within the perception algorithm. This integration of
registration and perception aligns with the trend towards end-      [1] D.-J. Lin, M.-Y. Chen, H.-S. Chiang, and P. K. Sharma, “Intelligent
                                                                        traffic accident prediction model for internet of vehicles with deep
to-end development in autonomous driving. However, these                learning approach,” IEEE transactions on intelligent transportation
methods still exhibit a low tolerance for deviations in external        systems, vol. 23, no. 3, pp. 2340–2349, 2021.
parameters. Under current research, multi-end spatial registra-     [2] X. Huang, P. Lin, C. Chen, B. Ran, and M. Tan, “Dynamic trajectory-
                                                                        based traffic dispersion method for intersection traffic accidents in an
tion remains a necessary step.                                          intelligent and connected environment,” IEEE Intelligent Transportation
                                                                        Systems Magazine, vol. 15, no. 1, pp. 84–100, 2021.
                   VIII. CONCLUSIONS                                [3] L. Luo, L. Sheng, H. Yu, and G. Sun, “Intersection-based v2x routing via
                                                                        reinforcement learning in vehicular ad hoc networks,” IEEE Transactions
   The main contribution of V2X-Reg++ is that it overcomes              on Intelligent Transportation Systems, vol. 23, no. 6, pp. 5446–5459,
the limitations of existing multi-end spatial registration meth-        2021.
 [4] M. Yang, B. Ai, R. He, Z. Ma, H. Mi, D. Fei, Z. Zhong, Y. Li, and J. Li,            IEEE/RSJ International Conference on Intelligent Robots and Systems
     “Dynamic v2v channel measurement and modeling at street intersection                (IROS). IEEE, 2020, pp. 9968–9975.
     scenarios,” IEEE Transactions on Antennas and Propagation, vol. 71,          [26]   Y. Xiong, X. Zhang, W. Gao, Y. Wang, J. Liu, Q. Qu, S. Guo, Y. Shen,
     no. 5, pp. 4417–4432, 2023.                                                         and J. Li, “Gf-slam: a novel hybrid localization method incorporating
 [5] P. Sun, D. Nam, R. Jayakrishnan, and W. Jin, “An eco-driving algorithm              global and arc features,” IEEE Transactions on Automation Science and
     based on vehicle to infrastructure (v2i) communications for signalized              Engineering, 2024.
     intersections,” Transportation Research Part C: Emerging Technologies,       [27]   X. Huang, G. Mei, J. Zhang, and R. Abbas, “A comprehensive survey
     vol. 144, p. 103876, 2022.                                                          on point cloud registration,” arXiv preprint arXiv:2103.02690, 2021.
 [6] R. Xu, H. Xiang, Z. Tu, X. Xia, M.-H. Yang, and J. Ma, “V2x-vit:             [28]   J. Yang, H. Li, D. Campbell, and Y. Jia, “Go-icp: A globally optimal
     Vehicle-to-everything cooperative perception with vision transformer,”              solution to 3d icp point-set registration,” IEEE transactions on pattern
     in European conference on computer vision. Springer, 2022, pp. 107–                 analysis and machine intelligence, vol. 38, no. 11, pp. 2241–2254, 2015.
     124.                                                                         [29]   K. Koide, M. Yokozuka, S. Oishi, and A. Banno, “Voxelized gicp for fast
 [7] Y. Hu, S. Fang, Z. Lei, Y. Zhong, and S. Chen, “Where2comm:                         and accurate 3d point cloud registration,” in 2021 IEEE International
     Communication-efficient collaborative perception via spatial confidence             Conference on Robotics and Automation (ICRA). IEEE, 2021, pp.
     maps,” Advances in neural information processing systems, vol. 35, pp.              11 054–11 059.
     4874–4886, 2022.                                                             [30]   Y. Zheng, Y. Li, S. Yang, and H. Lu, “Global-pbnet: A novel point cloud
 [8] Y. Xiong, X. Zhang, X. Gao, Q. Qu, C. Duan, R. Wang, J. Liu, and J. Li,             registration for autonomous driving,” IEEE Transactions on Intelligent
     “Cooperative camera-lidar extrinsic calibration for vehicle-infrastructure          Transportation Systems, vol. 23, no. 11, pp. 22 312–22 319, 2022.
     systems in urban intersections,” IEEE Internet of Things Journal, 2025.      [31]   C. Shi, X. Chen, K. Huang, J. Xiao, H. Lu, and C. Stachniss, “Keypoint
 [9] X. Zhang, Y. Xiong, Q. Qu, R. Wang, X. Gao, J. Liu, S. Guo, and J. Li,              matching for point cloud registration using multiplex dynamic graph
     “Cooperative visual-lidar extrinsic calibration technology for intersec-            attention networks,” IEEE Robotics and Automation Letters, vol. 6, no. 4,
     tion vehicle-infrastructure: A review,” arXiv preprint arXiv:2405.10132,            pp. 8221–8228, 2021.
     2024.                                                                        [32]   H. Yang, J. Shi, and L. Carlone, “Teaser: Fast and certifiable point cloud
[10] X. Zhang, Y. Xiong, Q. Qu, S. Zhu, S. Guo, D. Jin, G. Zhang, H. Ren,                registration,” IEEE Transactions on Robotics, vol. 37, no. 2, pp. 314–
     and J. Li, “Automated extrinsic calibration of multi-cameras and lidar,”            333, 2020.
     IEEE Transactions on Instrumentation and Measurement, 2023.                  [33]   Z. Qin, H. Yu, C. Wang, Y. Guo, Y. Peng, S. Ilic, D. Hu, and K. Xu,
[11] Y. Lu, Q. Li, B. Liu, M. Dianati, C. Feng, S. Chen, and Y. Wang,                    “Geotransformer: Fast and robust point cloud registration with geometric
     “Robust collaborative 3d object detection in presence of pose errors,”              transformer,” IEEE Transactions on Pattern Analysis and Machine
     in 2023 IEEE International Conference on Robotics and Automation                    Intelligence, vol. 45, no. 8, pp. 9806–9821, 2023.
     (ICRA). IEEE, 2023, pp. 4812–4818.                                           [34]   Q.-Y. Zhou, J. Park, and V. Koltun, “Fast global registration,” in Com-
[12] Z. Song, T. Xie, H. Zhang, J. Liu, F. Wen, and J. Li, “A spatial                    puter Vision–ECCV 2016: 14th European Conference, Amsterdam, The
     calibration method for robust cooperative perception,” IEEE Robotics                Netherlands, October 11-14, 2016, Proceedings, Part II 14. Springer,
     and Automation Letters, 2024.                                                       2016, pp. 766–782.
[13] Y. He, L. Ma, Z. Jiang, Y. Tang, and G. Xing, “Vi-eye: semantic-based 3d
                                                                                  [35]   P. Gao, R. Guo, H. Lu, and H. Z. Zhang, “Regularized graph matching
     point cloud registration for infrastructure-assisted autonomous driving,”
                                                                                         for correspondence identification under uncertainty in collaborative
     in Proceedings of the 27th Annual International Conference on Mobile
                                                                                         perception,” in Robotics science and systems, 2021.
     Computing and Networking, 2021, pp. 573–586.
                                                                                  [36]   C. Zhao, D. Ding, Y. Shi, Y. Ji, and Y. Du, “Graph matching-based
[14] S. Shi, J. Cui, Z. Jiang, Z. Yan, G. Xing, J. Niu, and Z. Ouyang, “Vips:
                                                                                         spatiotemporal calibration of roadside sensors in cooperative vehicle-
     Real-time perception fusion for infrastructure-assisted autonomous driv-
                                                                                         infrastructure systems,” IEEE Transactions on Intelligent Transportation
     ing,” in Proceedings of the 28th Annual International Conference on
                                                                                         Systems, 2024.
     Mobile Computing And Networking, 2022, pp. 133–146.
[15] N. Vadivelu, M. Ren, J. Tu, J. Wang, and R. Urtasun, “Learning to            [37]   C. Li, L. Xu, C. Jin, and L. Wang, “Graphps: Graph pair sequences-based
     communicate and correct pose errors,” in Conference on Robot Learning.              noisy-robust multi-hop collaborative perception,” IEEE Transactions on
     PMLR, 2021, pp. 1195–1210.                                                          Intelligent Vehicles, 2023.
[16] P. Xie and M. G. Petovello, “Measuring gnss multipath distributions in       [38]   C. Yang, Z. Zhou, H. Zhuang, C. Wang, and M. Yang, “Global pose
     urban canyon environments,” IEEE Transactions on Instrumentation and                initialization based on gridded gaussian distribution with wasserstein
     Measurement, vol. 64, no. 2, pp. 366–377, 2014.                                     distance,” IEEE Transactions on Intelligent Transportation Systems,
[17] M. Talas et al., “Connected vehicle pilot deployment program phase                  vol. 24, no. 5, pp. 5094–5104, 2023.
     3 – system performance report: New york city,” U.S. Department               [39]   Y. Aoki, H. Goforth, R. A. Srivatsan, and S. Lucey, “Pointnetlk: Robust
     of Transportation, Tech. Rep. FHWA-JPO-18-715, December 2021.                       & efficient point cloud registration using pointnet,” in Proceedings of
     [Online]. Available: https://rosap.ntl.bts.gov/view/dot/63102                       the IEEE/CVF conference on computer vision and pattern recognition,
[18] ——, “Connected vehicle pilot deployment program phase 3,                            2019, pp. 7163–7172.
     understanding and enabling cooperative driving for advanced connected        [40]   Y. Wang and J. M. Solomon, “Deep closest point: Learning represen-
     vehicles in new york city – new york city department of                             tations for point cloud registration,” in Proceedings of the IEEE/CVF
     transportation (nycdot),” U.S. Department of Transportation, Tech. Rep.             international conference on computer vision, 2019, pp. 3523–3532.
     FHWA-JPO-21-920, December 2021, table 74. [Online]. Available:               [41]   Y. Wu, H. Ding, M. Gong, A. K. Qin, W. Ma, Q. Miao, and K. C.
     https://rosap.ntl.bts.gov/view/dot/63613                                            Tan, “Evolutionary multiform optimization with two-stage bidirectional
[19] C. Sanders and Y. Wang, “Localizing spoofing attacks on vehicular                   knowledge transfer strategy for point cloud registration,” IEEE Trans-
     gps using vehicle-to-vehicle communications,” IEEE Transactions on                  actions on Evolutionary Computation, vol. 28, no. 1, pp. 62–76, 2024.
     Vehicular Technology, vol. 69, no. 12, pp. 15 656–15 667, 2020.              [42]   H. Ding, H. Xu, Y. Wu, H. Li, M. Gong, W. Ma, Q. Miao, J. Shi, and
[20] G. Twardokus and H. Rahbari, “Vehicle-to-nothing? securing c-v2x                    Y. Lei, “Evolutionary multitasking with two-level knowledge transfer
     against protocol-aware dos attacks,” in IEEE INFOCOM 2022-IEEE                      for multi-view point cloud registration,” in Proceedings of the Genetic
     Conference on Computer Communications. IEEE, 2022, pp. 1629–                        and Evolutionary Computation Conference, 2024, pp. 304–312.
     1638.                                                                        [43]   X. Huang, G. Mei, and J. Zhang, “Feature-metric registration: A fast
[21] F. Wang, Y. Hong, and X. Ban, “Infrastructure-enabled gps spoofing de-              semi-supervised approach for robust point cloud registration without cor-
     tection and correction,” IEEE Transactions on Intelligent Transportation            respondences,” in Proceedings of the IEEE/CVF conference on computer
     Systems, vol. 24, no. 12, pp. 13 878–13 892, 2023.                                  vision and pattern recognition, 2020, pp. 11 366–11 374.
[22] X. Huang, G. Mei, and J. Zhang, “Cross-source point cloud registration:      [44]   X. Wu, W. Li, D. Hong, R. Tao, and Q. Du, “Deep learning for
     Challenges, progress and prospects,” Neurocomputing, p. 126383, 2023.               unmanned aerial vehicle-based object detection and tracking: A survey,”
[23] Q. Qu, Y. Xiong, X. Wu, H. Li, and S. Guo, “V2i-calib: A novel                      IEEE Geoscience and Remote Sensing Magazine, vol. 10, no. 1, pp. 91–
     calibration approach for collaborative vehicle and infrastructure lidar             124, 2021.
     systems,” arXiv preprint arXiv:2407.10195, 2024.                             [45]   D. F. Crouse, “On implementing 2d rectangular assignment algorithms,”
[24] A. Dhall, K. Chelani, V. Radhakrishnan, and K. M. Krishna, “Lidar-                  IEEE Transactions on Aerospace and Electronic Systems, vol. 52, no. 4,
     camera calibration using 3d-3d point correspondences,” arXiv preprint               pp. 1679–1696, 2016.
     arXiv:1705.09785, 2017.                                                      [46]   K. S. Arun, T. S. Huang, and S. D. Blostein, “Least-squares fitting of
[25] J. Lv, J. Xu, K. Hu, Y. Liu, and X. Zuo, “Targetless calibration of                 two 3-d point sets,” IEEE Transactions on pattern analysis and machine
     lidar-imu system based on continuous-time batch estimation,” in 2020                intelligence, no. 5, pp. 698–700, 1987.
[47] Y. Zhao, X. Zhang, S. Zhang, S. Qiu, H. Yin, and X. Zhang, “Hpcr-vi:
     Heterogeneous point cloud registration for vehicle-infrastructure collab-
     oration,” in 2023 IEEE Intelligent Vehicles Symposium (IV). IEEE,
     2023, pp. 1–6.
[48] Y. Li, D. Ma, Z. An, Z. Wang, Y. Zhong, S. Chen, and C. Feng, “V2x-
     sim: Multi-agent collaborative perception dataset and benchmark for
     autonomous driving,” IEEE Robotics and Automation Letters, vol. 7,
     no. 4, pp. 10 914–10 921, 2022.
[49] H. Yu, W. Yang, H. Ruan, Z. Yang, Y. Tang, X. Gao, X. Hao, Y. Shi,
     Y. Pan, N. Sun, et al., “V2x-seq: A large-scale sequential dataset
     for vehicle-infrastructure cooperative perception and forecasting,” in
     Proceedings of the IEEE/CVF Conference on Computer Vision and
     Pattern Recognition, 2023, pp. 5486–5495.
[50] D. Krajzewicz, J. Erdmann, M. Behrisch, and L. Bieker, “Recent
     development and applications of sumo-simulation of urban mobility,”
     International journal on advances in systems and measurements, vol. 5,
     no. 3&4, 2012.
[51] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla:
     An open urban driving simulator,” in Conference on robot learning.
     PMLR, 2017, pp. 1–16.
[52] H. Lim, S. Yeon, S. Ryu, Y. Lee, Y. Kim, J. Yun, E. Jung, D. Lee, and
     H. Myung, “A single correspondence is enough: Robust global registra-
     tion to avoid degeneracy in urban environments,” in 2022 international
     conference on robotics and automation (ICRA). IEEE, 2022, pp. 8010–
     8017.
[53] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,
     “Pointpillars: Fast encoders for object detection from point clouds,”
     in Proceedings of the IEEE/CVF conference on computer vision and
     pattern recognition, 2019, pp. 12 697–12 705.
[54] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu,
     A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A
     multimodal dataset for autonomous driving,” in Proceedings of the
     IEEE/CVF conference on computer vision and pattern recognition, 2020,
     pp. 11 621–11 631.
[55] P. J. Besl and N. D. McKay, “Method for registration of 3-d shapes,”
     in Sensor fusion IV: control paradigms and data structures, vol. 1611.
     Spie, 1992, pp. 586–606.
[56] J. Serafin and G. Grisetti, “Using extended measurements and scene
     merging for efficient and robust point cloud registration,” Robotics and
     Autonomous Systems, vol. 92, pp. 91–106, 2017.
[57] Y. Yan, Y. Mao, and B. Li, “Second: Sparsely embedded convolutional
     detection,” Sensors, vol. 18, no. 10, p. 3337, 2018.
[58] P. J. Besl and N. D. McKay, “Method for registration of 3-d shapes,”
     in Sensor fusion IV: control paradigms and data structures, vol. 1611.
     Spie, 1992, pp. 586–606.
[59] N. Bodla, B. Singh, R. Chellappa, and L. S. Davis, “Soft-nms–improving
     object detection with one line of code,” in Proceedings of the IEEE
     international conference on computer vision, 2017, pp. 5561–5569.
[60] T.-H. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, and
     R. Urtasun, “V2vnet: Vehicle-to-vehicle communication for joint percep-
     tion and prediction,” in Computer Vision–ECCV 2020: 16th European
     Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16.
     Springer, 2020, pp. 605–621.
[61] Y. Yuan, H. Cheng, and M. Sester, “Keypoints-based deep feature fusion
     for cooperative vehicle detection of autonomous driving,” IEEE Robotics
     and Automation Letters, vol. 7, no. 2, pp. 3054–3061, 2022.
